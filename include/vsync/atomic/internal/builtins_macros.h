/*
 * Copyright (C) Huawei Technologies Co., Ltd. 2023-2024. All rights reserved.
 * Author: Huawei Dresden Research Center
 */
#ifndef VATOMIC_BUILTINS_H
#define VATOMIC_BUILTINS_H
/* !!!Warning: File generated by tmpl; DO NOT EDIT.!!! */
#include <vsync/common/macros.h>

/* ****************************************************************************
 * vatomic_fence
 * ****************************************************************************/

#ifndef VATOMIC_FENCE
    #define VATOMIC_FENCE

    #define vatomic_fence()                                                    \
        __atomic_thread_fence(V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC_FENCE */

#ifndef VATOMIC_FENCE_ACQ
    #define VATOMIC_FENCE_ACQ

    #define vatomic_fence_acq()                                                \
        __atomic_thread_fence(V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC_FENCE_ACQ */

#ifndef VATOMIC_FENCE_REL
    #define VATOMIC_FENCE_REL

    #define vatomic_fence_rel()                                                \
        __atomic_thread_fence(V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC_FENCE_REL */

#ifndef VATOMIC_FENCE_RLX
    #define VATOMIC_FENCE_RLX

    #define vatomic_fence_rlx()                                                \
        __atomic_thread_fence(V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC_FENCE_RLX */

/* *****************************************************************************
 * vatomic_read
 * ****************************************************************************/

#ifndef VATOMIC8_READ
    #define VATOMIC8_READ

    #define vatomic8_read(a)                                                   \
        (vuint8_t) __atomic_load_n(                                            \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_READ */

#ifndef VATOMIC8_READ_ACQ
    #define VATOMIC8_READ_ACQ

    #define vatomic8_read_acq(a)                                               \
        (vuint8_t) __atomic_load_n(                                            \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC8_READ_ACQ */

#ifndef VATOMIC8_READ_RLX
    #define VATOMIC8_READ_RLX

    #define vatomic8_read_rlx(a)                                               \
        (vuint8_t) __atomic_load_n(                                            \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_READ_RLX */

#ifndef VATOMIC16_READ
    #define VATOMIC16_READ

    #define vatomic16_read(a)                                                  \
        (vuint16_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_READ */

#ifndef VATOMIC16_READ_ACQ
    #define VATOMIC16_READ_ACQ

    #define vatomic16_read_acq(a)                                              \
        (vuint16_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC16_READ_ACQ */

#ifndef VATOMIC16_READ_RLX
    #define VATOMIC16_READ_RLX

    #define vatomic16_read_rlx(a)                                              \
        (vuint16_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_READ_RLX */

#ifndef VATOMIC32_READ
    #define VATOMIC32_READ

    #define vatomic32_read(a)                                                  \
        (vuint32_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_READ */

#ifndef VATOMIC32_READ_ACQ
    #define VATOMIC32_READ_ACQ

    #define vatomic32_read_acq(a)                                              \
        (vuint32_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC32_READ_ACQ */

#ifndef VATOMIC32_READ_RLX
    #define VATOMIC32_READ_RLX

    #define vatomic32_read_rlx(a)                                              \
        (vuint32_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_READ_RLX */

#ifndef VATOMIC64_READ
    #define VATOMIC64_READ

    #define vatomic64_read(a)                                                  \
        (vuint64_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_READ */

#ifndef VATOMIC64_READ_ACQ
    #define VATOMIC64_READ_ACQ

    #define vatomic64_read_acq(a)                                              \
        (vuint64_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC64_READ_ACQ */

#ifndef VATOMIC64_READ_RLX
    #define VATOMIC64_READ_RLX

    #define vatomic64_read_rlx(a)                                              \
        (vuint64_t) __atomic_load_n(                                           \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_READ_RLX */

#ifndef VATOMICSZ_READ
    #define VATOMICSZ_READ

    #define vatomicsz_read(a)                                                  \
        (vsize_t) __atomic_load_n(                                             \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_READ */

#ifndef VATOMICSZ_READ_ACQ
    #define VATOMICSZ_READ_ACQ

    #define vatomicsz_read_acq(a)                                              \
        (vsize_t) __atomic_load_n(                                             \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICSZ_READ_ACQ */

#ifndef VATOMICSZ_READ_RLX
    #define VATOMICSZ_READ_RLX

    #define vatomicsz_read_rlx(a)                                              \
        (vsize_t) __atomic_load_n(                                             \
            &(a)->_v, V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_READ_RLX */

#ifndef VATOMICPTR_READ
    #define VATOMICPTR_READ

    #define vatomicptr_read(a)                                                 \
        (void *)__atomic_load_n(&(a)->_v,                                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICPTR_READ */

#ifndef VATOMICPTR_READ_ACQ
    #define VATOMICPTR_READ_ACQ

    #define vatomicptr_read_acq(a)                                             \
        (void *)__atomic_load_n(&(a)->_v,                                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICPTR_READ_ACQ */

#ifndef VATOMICPTR_READ_RLX
    #define VATOMICPTR_READ_RLX

    #define vatomicptr_read_rlx(a)                                             \
        (void *)__atomic_load_n(&(a)->_v,                                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICPTR_READ_RLX */

/* *****************************************************************************
 * vatomic_write
 * ****************************************************************************/

#ifndef VATOMIC8_WRITE
    #define VATOMIC8_WRITE

    #define vatomic8_write(a, v)                                               \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_WRITE */

#ifndef VATOMIC8_WRITE_REL
    #define VATOMIC8_WRITE_REL

    #define vatomic8_write_rel(a, v)                                           \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC8_WRITE_REL */

#ifndef VATOMIC8_WRITE_RLX
    #define VATOMIC8_WRITE_RLX

    #define vatomic8_write_rlx(a, v)                                           \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_WRITE_RLX */

#ifndef VATOMIC16_WRITE
    #define VATOMIC16_WRITE

    #define vatomic16_write(a, v)                                              \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_WRITE */

#ifndef VATOMIC16_WRITE_REL
    #define VATOMIC16_WRITE_REL

    #define vatomic16_write_rel(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC16_WRITE_REL */

#ifndef VATOMIC16_WRITE_RLX
    #define VATOMIC16_WRITE_RLX

    #define vatomic16_write_rlx(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_WRITE_RLX */

#ifndef VATOMIC32_WRITE
    #define VATOMIC32_WRITE

    #define vatomic32_write(a, v)                                              \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_WRITE */

#ifndef VATOMIC32_WRITE_REL
    #define VATOMIC32_WRITE_REL

    #define vatomic32_write_rel(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC32_WRITE_REL */

#ifndef VATOMIC32_WRITE_RLX
    #define VATOMIC32_WRITE_RLX

    #define vatomic32_write_rlx(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_WRITE_RLX */

#ifndef VATOMIC64_WRITE
    #define VATOMIC64_WRITE

    #define vatomic64_write(a, v)                                              \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_WRITE */

#ifndef VATOMIC64_WRITE_REL
    #define VATOMIC64_WRITE_REL

    #define vatomic64_write_rel(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC64_WRITE_REL */

#ifndef VATOMIC64_WRITE_RLX
    #define VATOMIC64_WRITE_RLX

    #define vatomic64_write_rlx(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_WRITE_RLX */

#ifndef VATOMICSZ_WRITE
    #define VATOMICSZ_WRITE

    #define vatomicsz_write(a, v)                                              \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_WRITE */

#ifndef VATOMICSZ_WRITE_REL
    #define VATOMICSZ_WRITE_REL

    #define vatomicsz_write_rel(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICSZ_WRITE_REL */

#ifndef VATOMICSZ_WRITE_RLX
    #define VATOMICSZ_WRITE_RLX

    #define vatomicsz_write_rlx(a, v)                                          \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_WRITE_RLX */

#ifndef VATOMICPTR_WRITE
    #define VATOMICPTR_WRITE

    #define vatomicptr_write(a, v)                                             \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICPTR_WRITE */

#ifndef VATOMICPTR_WRITE_REL
    #define VATOMICPTR_WRITE_REL

    #define vatomicptr_write_rel(a, v)                                         \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICPTR_WRITE_REL */

#ifndef VATOMICPTR_WRITE_RLX
    #define VATOMICPTR_WRITE_RLX

    #define vatomicptr_write_rlx(a, v)                                         \
        __atomic_store_n(&(a)->_v, (v),                                        \
                         V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICPTR_WRITE_RLX */

/* *****************************************************************************
 * vatomic_xchg
 * ****************************************************************************/

#ifndef VATOMIC8_XCHG
    #define VATOMIC8_XCHG

    #define vatomic8_xchg(a, v)                                                \
        (vuint8_t)                                                             \
            __atomic_exchange_n(&(a)->_v, (vuint8_t)(v),                       \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_XCHG */

#ifndef VATOMIC8_XCHG_ACQ
    #define VATOMIC8_XCHG_ACQ

    #define vatomic8_xchg_acq(a, v)                                            \
        (vuint8_t)                                                             \
            __atomic_exchange_n(&(a)->_v, (vuint8_t)(v),                       \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC8_XCHG_ACQ */

#ifndef VATOMIC8_XCHG_REL
    #define VATOMIC8_XCHG_REL

    #define vatomic8_xchg_rel(a, v)                                            \
        (vuint8_t)                                                             \
            __atomic_exchange_n(&(a)->_v, (vuint8_t)(v),                       \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC8_XCHG_REL */

#ifndef VATOMIC8_XCHG_RLX
    #define VATOMIC8_XCHG_RLX

    #define vatomic8_xchg_rlx(a, v)                                            \
        (vuint8_t)                                                             \
            __atomic_exchange_n(&(a)->_v, (vuint8_t)(v),                       \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_XCHG_RLX */

#ifndef VATOMIC16_XCHG
    #define VATOMIC16_XCHG

    #define vatomic16_xchg(a, v)                                               \
        (vuint16_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint16_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_XCHG */

#ifndef VATOMIC16_XCHG_ACQ
    #define VATOMIC16_XCHG_ACQ

    #define vatomic16_xchg_acq(a, v)                                           \
        (vuint16_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint16_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC16_XCHG_ACQ */

#ifndef VATOMIC16_XCHG_REL
    #define VATOMIC16_XCHG_REL

    #define vatomic16_xchg_rel(a, v)                                           \
        (vuint16_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint16_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC16_XCHG_REL */

#ifndef VATOMIC16_XCHG_RLX
    #define VATOMIC16_XCHG_RLX

    #define vatomic16_xchg_rlx(a, v)                                           \
        (vuint16_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint16_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_XCHG_RLX */

#ifndef VATOMIC32_XCHG
    #define VATOMIC32_XCHG

    #define vatomic32_xchg(a, v)                                               \
        (vuint32_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint32_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_XCHG */

#ifndef VATOMIC32_XCHG_ACQ
    #define VATOMIC32_XCHG_ACQ

    #define vatomic32_xchg_acq(a, v)                                           \
        (vuint32_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint32_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC32_XCHG_ACQ */

#ifndef VATOMIC32_XCHG_REL
    #define VATOMIC32_XCHG_REL

    #define vatomic32_xchg_rel(a, v)                                           \
        (vuint32_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint32_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC32_XCHG_REL */

#ifndef VATOMIC32_XCHG_RLX
    #define VATOMIC32_XCHG_RLX

    #define vatomic32_xchg_rlx(a, v)                                           \
        (vuint32_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint32_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_XCHG_RLX */

#ifndef VATOMIC64_XCHG
    #define VATOMIC64_XCHG

    #define vatomic64_xchg(a, v)                                               \
        (vuint64_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint64_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_XCHG */

#ifndef VATOMIC64_XCHG_ACQ
    #define VATOMIC64_XCHG_ACQ

    #define vatomic64_xchg_acq(a, v)                                           \
        (vuint64_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint64_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC64_XCHG_ACQ */

#ifndef VATOMIC64_XCHG_REL
    #define VATOMIC64_XCHG_REL

    #define vatomic64_xchg_rel(a, v)                                           \
        (vuint64_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint64_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC64_XCHG_REL */

#ifndef VATOMIC64_XCHG_RLX
    #define VATOMIC64_XCHG_RLX

    #define vatomic64_xchg_rlx(a, v)                                           \
        (vuint64_t)                                                            \
            __atomic_exchange_n(&(a)->_v, (vuint64_t)(v),                      \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_XCHG_RLX */

#ifndef VATOMICSZ_XCHG
    #define VATOMICSZ_XCHG

    #define vatomicsz_xchg(a, v)                                               \
        (vsize_t)                                                              \
            __atomic_exchange_n(&(a)->_v, (vsize_t)(v),                        \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_XCHG */

#ifndef VATOMICSZ_XCHG_ACQ
    #define VATOMICSZ_XCHG_ACQ

    #define vatomicsz_xchg_acq(a, v)                                           \
        (vsize_t)                                                              \
            __atomic_exchange_n(&(a)->_v, (vsize_t)(v),                        \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICSZ_XCHG_ACQ */

#ifndef VATOMICSZ_XCHG_REL
    #define VATOMICSZ_XCHG_REL

    #define vatomicsz_xchg_rel(a, v)                                           \
        (vsize_t)                                                              \
            __atomic_exchange_n(&(a)->_v, (vsize_t)(v),                        \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICSZ_XCHG_REL */

#ifndef VATOMICSZ_XCHG_RLX
    #define VATOMICSZ_XCHG_RLX

    #define vatomicsz_xchg_rlx(a, v)                                           \
        (vsize_t)                                                              \
            __atomic_exchange_n(&(a)->_v, (vsize_t)(v),                        \
                                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_XCHG_RLX */

#ifndef VATOMICPTR_XCHG
    #define VATOMICPTR_XCHG

    #define vatomicptr_xchg(a, v)                                              \
        (void *)__atomic_exchange_n(                                           \
            &(a)->_v, (void *)(v),                                             \
            V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICPTR_XCHG */

#ifndef VATOMICPTR_XCHG_ACQ
    #define VATOMICPTR_XCHG_ACQ

    #define vatomicptr_xchg_acq(a, v)                                          \
        (void *)__atomic_exchange_n(                                           \
            &(a)->_v, (void *)(v),                                             \
            V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICPTR_XCHG_ACQ */

#ifndef VATOMICPTR_XCHG_REL
    #define VATOMICPTR_XCHG_REL

    #define vatomicptr_xchg_rel(a, v)                                          \
        (void *)__atomic_exchange_n(                                           \
            &(a)->_v, (void *)(v),                                             \
            V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICPTR_XCHG_REL */

#ifndef VATOMICPTR_XCHG_RLX
    #define VATOMICPTR_XCHG_RLX

    #define vatomicptr_xchg_rlx(a, v)                                          \
        (void *)__atomic_exchange_n(                                           \
            &(a)->_v, (void *)(v),                                             \
            V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICPTR_XCHG_RLX */

/* *****************************************************************************
 * vatomic_cmpxchg
 * ****************************************************************************/

#ifndef VATOMIC8_CMPXCHG
    #define VATOMIC8_CMPXCHG

    #define vatomic8_cmpxchg(a, e, v)                                          \
        ({                                                                     \
            vuint8_t _e_ = (vuint8_t)(e);                                      \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint8_t)(v), 0,                              \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC8_CMPXCHG */

#ifndef VATOMIC8_CMPXCHG_ACQ
    #define VATOMIC8_CMPXCHG_ACQ

    #define vatomic8_cmpxchg_acq(a, e, v)                                      \
        ({                                                                     \
            vuint8_t _e_ = (vuint8_t)(e);                                      \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint8_t)(v), 0,                              \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC8_CMPXCHG_ACQ */

#ifndef VATOMIC8_CMPXCHG_REL
    #define VATOMIC8_CMPXCHG_REL

    #define vatomic8_cmpxchg_rel(a, e, v)                                      \
        ({                                                                     \
            vuint8_t _e_ = (vuint8_t)(e);                                      \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint8_t)(v), 0,                              \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC8_CMPXCHG_REL */

#ifndef VATOMIC8_CMPXCHG_RLX
    #define VATOMIC8_CMPXCHG_RLX

    #define vatomic8_cmpxchg_rlx(a, e, v)                                      \
        ({                                                                     \
            vuint8_t _e_ = (vuint8_t)(e);                                      \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint8_t)(v), 0,                              \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC8_CMPXCHG_RLX */

#ifndef VATOMIC16_CMPXCHG
    #define VATOMIC16_CMPXCHG

    #define vatomic16_cmpxchg(a, e, v)                                         \
        ({                                                                     \
            vuint16_t _e_ = (vuint16_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint16_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC16_CMPXCHG */

#ifndef VATOMIC16_CMPXCHG_ACQ
    #define VATOMIC16_CMPXCHG_ACQ

    #define vatomic16_cmpxchg_acq(a, e, v)                                     \
        ({                                                                     \
            vuint16_t _e_ = (vuint16_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint16_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC16_CMPXCHG_ACQ */

#ifndef VATOMIC16_CMPXCHG_REL
    #define VATOMIC16_CMPXCHG_REL

    #define vatomic16_cmpxchg_rel(a, e, v)                                     \
        ({                                                                     \
            vuint16_t _e_ = (vuint16_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint16_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC16_CMPXCHG_REL */

#ifndef VATOMIC16_CMPXCHG_RLX
    #define VATOMIC16_CMPXCHG_RLX

    #define vatomic16_cmpxchg_rlx(a, e, v)                                     \
        ({                                                                     \
            vuint16_t _e_ = (vuint16_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint16_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC16_CMPXCHG_RLX */

#ifndef VATOMIC32_CMPXCHG
    #define VATOMIC32_CMPXCHG

    #define vatomic32_cmpxchg(a, e, v)                                         \
        ({                                                                     \
            vuint32_t _e_ = (vuint32_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint32_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC32_CMPXCHG */

#ifndef VATOMIC32_CMPXCHG_ACQ
    #define VATOMIC32_CMPXCHG_ACQ

    #define vatomic32_cmpxchg_acq(a, e, v)                                     \
        ({                                                                     \
            vuint32_t _e_ = (vuint32_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint32_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC32_CMPXCHG_ACQ */

#ifndef VATOMIC32_CMPXCHG_REL
    #define VATOMIC32_CMPXCHG_REL

    #define vatomic32_cmpxchg_rel(a, e, v)                                     \
        ({                                                                     \
            vuint32_t _e_ = (vuint32_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint32_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC32_CMPXCHG_REL */

#ifndef VATOMIC32_CMPXCHG_RLX
    #define VATOMIC32_CMPXCHG_RLX

    #define vatomic32_cmpxchg_rlx(a, e, v)                                     \
        ({                                                                     \
            vuint32_t _e_ = (vuint32_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint32_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC32_CMPXCHG_RLX */

#ifndef VATOMIC64_CMPXCHG
    #define VATOMIC64_CMPXCHG

    #define vatomic64_cmpxchg(a, e, v)                                         \
        ({                                                                     \
            vuint64_t _e_ = (vuint64_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint64_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC64_CMPXCHG */

#ifndef VATOMIC64_CMPXCHG_ACQ
    #define VATOMIC64_CMPXCHG_ACQ

    #define vatomic64_cmpxchg_acq(a, e, v)                                     \
        ({                                                                     \
            vuint64_t _e_ = (vuint64_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint64_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC64_CMPXCHG_ACQ */

#ifndef VATOMIC64_CMPXCHG_REL
    #define VATOMIC64_CMPXCHG_REL

    #define vatomic64_cmpxchg_rel(a, e, v)                                     \
        ({                                                                     \
            vuint64_t _e_ = (vuint64_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint64_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC64_CMPXCHG_REL */

#ifndef VATOMIC64_CMPXCHG_RLX
    #define VATOMIC64_CMPXCHG_RLX

    #define vatomic64_cmpxchg_rlx(a, e, v)                                     \
        ({                                                                     \
            vuint64_t _e_ = (vuint64_t)(e);                                    \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vuint64_t)(v), 0,                             \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMIC64_CMPXCHG_RLX */

#ifndef VATOMICSZ_CMPXCHG
    #define VATOMICSZ_CMPXCHG

    #define vatomicsz_cmpxchg(a, e, v)                                         \
        ({                                                                     \
            vsize_t _e_ = (vsize_t)(e);                                        \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vsize_t)(v), 0,                               \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST));                 \
            _e_;                                                               \
        })

#endif /* VATOMICSZ_CMPXCHG */

#ifndef VATOMICSZ_CMPXCHG_ACQ
    #define VATOMICSZ_CMPXCHG_ACQ

    #define vatomicsz_cmpxchg_acq(a, e, v)                                     \
        ({                                                                     \
            vsize_t _e_ = (vsize_t)(e);                                        \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vsize_t)(v), 0,                               \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE));                 \
            _e_;                                                               \
        })

#endif /* VATOMICSZ_CMPXCHG_ACQ */

#ifndef VATOMICSZ_CMPXCHG_REL
    #define VATOMICSZ_CMPXCHG_REL

    #define vatomicsz_cmpxchg_rel(a, e, v)                                     \
        ({                                                                     \
            vsize_t _e_ = (vsize_t)(e);                                        \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vsize_t)(v), 0,                               \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMICSZ_CMPXCHG_REL */

#ifndef VATOMICSZ_CMPXCHG_RLX
    #define VATOMICSZ_CMPXCHG_RLX

    #define vatomicsz_cmpxchg_rlx(a, e, v)                                     \
        ({                                                                     \
            vsize_t _e_ = (vsize_t)(e);                                        \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (vsize_t)(v), 0,                               \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMICSZ_CMPXCHG_RLX */

#ifndef VATOMICPTR_CMPXCHG
    #define VATOMICPTR_CMPXCHG

    #define vatomicptr_cmpxchg(a, e, v)                                        \
        ({                                                                     \
            void *_e_ = (void *)(e);                                           \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (void *)(v), 0,                                \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST));                 \
            _e_;                                                               \
        })

#endif /* VATOMICPTR_CMPXCHG */

#ifndef VATOMICPTR_CMPXCHG_ACQ
    #define VATOMICPTR_CMPXCHG_ACQ

    #define vatomicptr_cmpxchg_acq(a, e, v)                                    \
        ({                                                                     \
            void *_e_ = (void *)(e);                                           \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (void *)(v), 0,                                \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE));                 \
            _e_;                                                               \
        })

#endif /* VATOMICPTR_CMPXCHG_ACQ */

#ifndef VATOMICPTR_CMPXCHG_REL
    #define VATOMICPTR_CMPXCHG_REL

    #define vatomicptr_cmpxchg_rel(a, e, v)                                    \
        ({                                                                     \
            void *_e_ = (void *)(e);                                           \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (void *)(v), 0,                                \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMICPTR_CMPXCHG_REL */

#ifndef VATOMICPTR_CMPXCHG_RLX
    #define VATOMICPTR_CMPXCHG_RLX

    #define vatomicptr_cmpxchg_rlx(a, e, v)                                    \
        ({                                                                     \
            void *_e_ = (void *)(e);                                           \
            __atomic_compare_exchange_n(                                       \
                &(a)->_v, &_e_, (void *)(v), 0,                                \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED),                  \
                V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED));                 \
            _e_;                                                               \
        })

#endif /* VATOMICPTR_CMPXCHG_RLX */

/* *****************************************************************************
 * vatomic_get_and
 * ****************************************************************************/

#ifndef VATOMIC8_GET_AND
    #define VATOMIC8_GET_AND

    #define vatomic8_get_and(a, v)                                             \
        (vuint8_t)                                                             \
            __atomic_fetch_and(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_GET_AND */

#ifndef VATOMIC8_GET_AND_ACQ
    #define VATOMIC8_GET_AND_ACQ

    #define vatomic8_get_and_acq(a, v)                                         \
        (vuint8_t)                                                             \
            __atomic_fetch_and(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC8_GET_AND_ACQ */

#ifndef VATOMIC8_GET_AND_REL
    #define VATOMIC8_GET_AND_REL

    #define vatomic8_get_and_rel(a, v)                                         \
        (vuint8_t)                                                             \
            __atomic_fetch_and(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC8_GET_AND_REL */

#ifndef VATOMIC8_GET_AND_RLX
    #define VATOMIC8_GET_AND_RLX

    #define vatomic8_get_and_rlx(a, v)                                         \
        (vuint8_t)                                                             \
            __atomic_fetch_and(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_GET_AND_RLX */

#ifndef VATOMIC16_GET_AND
    #define VATOMIC16_GET_AND

    #define vatomic16_get_and(a, v)                                            \
        (vuint16_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_GET_AND */

#ifndef VATOMIC16_GET_AND_ACQ
    #define VATOMIC16_GET_AND_ACQ

    #define vatomic16_get_and_acq(a, v)                                        \
        (vuint16_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC16_GET_AND_ACQ */

#ifndef VATOMIC16_GET_AND_REL
    #define VATOMIC16_GET_AND_REL

    #define vatomic16_get_and_rel(a, v)                                        \
        (vuint16_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC16_GET_AND_REL */

#ifndef VATOMIC16_GET_AND_RLX
    #define VATOMIC16_GET_AND_RLX

    #define vatomic16_get_and_rlx(a, v)                                        \
        (vuint16_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_GET_AND_RLX */

#ifndef VATOMIC32_GET_AND
    #define VATOMIC32_GET_AND

    #define vatomic32_get_and(a, v)                                            \
        (vuint32_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_GET_AND */

#ifndef VATOMIC32_GET_AND_ACQ
    #define VATOMIC32_GET_AND_ACQ

    #define vatomic32_get_and_acq(a, v)                                        \
        (vuint32_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC32_GET_AND_ACQ */

#ifndef VATOMIC32_GET_AND_REL
    #define VATOMIC32_GET_AND_REL

    #define vatomic32_get_and_rel(a, v)                                        \
        (vuint32_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC32_GET_AND_REL */

#ifndef VATOMIC32_GET_AND_RLX
    #define VATOMIC32_GET_AND_RLX

    #define vatomic32_get_and_rlx(a, v)                                        \
        (vuint32_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_GET_AND_RLX */

#ifndef VATOMIC64_GET_AND
    #define VATOMIC64_GET_AND

    #define vatomic64_get_and(a, v)                                            \
        (vuint64_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_GET_AND */

#ifndef VATOMIC64_GET_AND_ACQ
    #define VATOMIC64_GET_AND_ACQ

    #define vatomic64_get_and_acq(a, v)                                        \
        (vuint64_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC64_GET_AND_ACQ */

#ifndef VATOMIC64_GET_AND_REL
    #define VATOMIC64_GET_AND_REL

    #define vatomic64_get_and_rel(a, v)                                        \
        (vuint64_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC64_GET_AND_REL */

#ifndef VATOMIC64_GET_AND_RLX
    #define VATOMIC64_GET_AND_RLX

    #define vatomic64_get_and_rlx(a, v)                                        \
        (vuint64_t)                                                            \
            __atomic_fetch_and(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_GET_AND_RLX */

#ifndef VATOMICSZ_GET_AND
    #define VATOMICSZ_GET_AND

    #define vatomicsz_get_and(a, v)                                            \
        (vsize_t)                                                              \
            __atomic_fetch_and(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_GET_AND */

#ifndef VATOMICSZ_GET_AND_ACQ
    #define VATOMICSZ_GET_AND_ACQ

    #define vatomicsz_get_and_acq(a, v)                                        \
        (vsize_t)                                                              \
            __atomic_fetch_and(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICSZ_GET_AND_ACQ */

#ifndef VATOMICSZ_GET_AND_REL
    #define VATOMICSZ_GET_AND_REL

    #define vatomicsz_get_and_rel(a, v)                                        \
        (vsize_t)                                                              \
            __atomic_fetch_and(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICSZ_GET_AND_REL */

#ifndef VATOMICSZ_GET_AND_RLX
    #define VATOMICSZ_GET_AND_RLX

    #define vatomicsz_get_and_rlx(a, v)                                        \
        (vsize_t)                                                              \
            __atomic_fetch_and(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_GET_AND_RLX */

/* *****************************************************************************
 * vatomic_get_or
 * ****************************************************************************/

#ifndef VATOMIC8_GET_OR
    #define VATOMIC8_GET_OR

    #define vatomic8_get_or(a, v)                                              \
        (vuint8_t)                                                             \
            __atomic_fetch_or(&(a)->_v, (vuint8_t)(v),                         \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_GET_OR */

#ifndef VATOMIC8_GET_OR_ACQ
    #define VATOMIC8_GET_OR_ACQ

    #define vatomic8_get_or_acq(a, v)                                          \
        (vuint8_t)                                                             \
            __atomic_fetch_or(&(a)->_v, (vuint8_t)(v),                         \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC8_GET_OR_ACQ */

#ifndef VATOMIC8_GET_OR_REL
    #define VATOMIC8_GET_OR_REL

    #define vatomic8_get_or_rel(a, v)                                          \
        (vuint8_t)                                                             \
            __atomic_fetch_or(&(a)->_v, (vuint8_t)(v),                         \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC8_GET_OR_REL */

#ifndef VATOMIC8_GET_OR_RLX
    #define VATOMIC8_GET_OR_RLX

    #define vatomic8_get_or_rlx(a, v)                                          \
        (vuint8_t)                                                             \
            __atomic_fetch_or(&(a)->_v, (vuint8_t)(v),                         \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_GET_OR_RLX */

#ifndef VATOMIC16_GET_OR
    #define VATOMIC16_GET_OR

    #define vatomic16_get_or(a, v)                                             \
        (vuint16_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint16_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_GET_OR */

#ifndef VATOMIC16_GET_OR_ACQ
    #define VATOMIC16_GET_OR_ACQ

    #define vatomic16_get_or_acq(a, v)                                         \
        (vuint16_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint16_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC16_GET_OR_ACQ */

#ifndef VATOMIC16_GET_OR_REL
    #define VATOMIC16_GET_OR_REL

    #define vatomic16_get_or_rel(a, v)                                         \
        (vuint16_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint16_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC16_GET_OR_REL */

#ifndef VATOMIC16_GET_OR_RLX
    #define VATOMIC16_GET_OR_RLX

    #define vatomic16_get_or_rlx(a, v)                                         \
        (vuint16_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint16_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_GET_OR_RLX */

#ifndef VATOMIC32_GET_OR
    #define VATOMIC32_GET_OR

    #define vatomic32_get_or(a, v)                                             \
        (vuint32_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint32_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_GET_OR */

#ifndef VATOMIC32_GET_OR_ACQ
    #define VATOMIC32_GET_OR_ACQ

    #define vatomic32_get_or_acq(a, v)                                         \
        (vuint32_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint32_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC32_GET_OR_ACQ */

#ifndef VATOMIC32_GET_OR_REL
    #define VATOMIC32_GET_OR_REL

    #define vatomic32_get_or_rel(a, v)                                         \
        (vuint32_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint32_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC32_GET_OR_REL */

#ifndef VATOMIC32_GET_OR_RLX
    #define VATOMIC32_GET_OR_RLX

    #define vatomic32_get_or_rlx(a, v)                                         \
        (vuint32_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint32_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_GET_OR_RLX */

#ifndef VATOMIC64_GET_OR
    #define VATOMIC64_GET_OR

    #define vatomic64_get_or(a, v)                                             \
        (vuint64_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint64_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_GET_OR */

#ifndef VATOMIC64_GET_OR_ACQ
    #define VATOMIC64_GET_OR_ACQ

    #define vatomic64_get_or_acq(a, v)                                         \
        (vuint64_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint64_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC64_GET_OR_ACQ */

#ifndef VATOMIC64_GET_OR_REL
    #define VATOMIC64_GET_OR_REL

    #define vatomic64_get_or_rel(a, v)                                         \
        (vuint64_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint64_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC64_GET_OR_REL */

#ifndef VATOMIC64_GET_OR_RLX
    #define VATOMIC64_GET_OR_RLX

    #define vatomic64_get_or_rlx(a, v)                                         \
        (vuint64_t)                                                            \
            __atomic_fetch_or(&(a)->_v, (vuint64_t)(v),                        \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_GET_OR_RLX */

#ifndef VATOMICSZ_GET_OR
    #define VATOMICSZ_GET_OR

    #define vatomicsz_get_or(a, v)                                             \
        (vsize_t)                                                              \
            __atomic_fetch_or(&(a)->_v, (vsize_t)(v),                          \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_GET_OR */

#ifndef VATOMICSZ_GET_OR_ACQ
    #define VATOMICSZ_GET_OR_ACQ

    #define vatomicsz_get_or_acq(a, v)                                         \
        (vsize_t)                                                              \
            __atomic_fetch_or(&(a)->_v, (vsize_t)(v),                          \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICSZ_GET_OR_ACQ */

#ifndef VATOMICSZ_GET_OR_REL
    #define VATOMICSZ_GET_OR_REL

    #define vatomicsz_get_or_rel(a, v)                                         \
        (vsize_t)                                                              \
            __atomic_fetch_or(&(a)->_v, (vsize_t)(v),                          \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICSZ_GET_OR_REL */

#ifndef VATOMICSZ_GET_OR_RLX
    #define VATOMICSZ_GET_OR_RLX

    #define vatomicsz_get_or_rlx(a, v)                                         \
        (vsize_t)                                                              \
            __atomic_fetch_or(&(a)->_v, (vsize_t)(v),                          \
                              V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_GET_OR_RLX */

/* *****************************************************************************
 * vatomic_get_xor
 * ****************************************************************************/

#ifndef VATOMIC8_GET_XOR
    #define VATOMIC8_GET_XOR

    #define vatomic8_get_xor(a, v)                                             \
        (vuint8_t)                                                             \
            __atomic_fetch_xor(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_GET_XOR */

#ifndef VATOMIC8_GET_XOR_ACQ
    #define VATOMIC8_GET_XOR_ACQ

    #define vatomic8_get_xor_acq(a, v)                                         \
        (vuint8_t)                                                             \
            __atomic_fetch_xor(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC8_GET_XOR_ACQ */

#ifndef VATOMIC8_GET_XOR_REL
    #define VATOMIC8_GET_XOR_REL

    #define vatomic8_get_xor_rel(a, v)                                         \
        (vuint8_t)                                                             \
            __atomic_fetch_xor(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC8_GET_XOR_REL */

#ifndef VATOMIC8_GET_XOR_RLX
    #define VATOMIC8_GET_XOR_RLX

    #define vatomic8_get_xor_rlx(a, v)                                         \
        (vuint8_t)                                                             \
            __atomic_fetch_xor(&(a)->_v, (vuint8_t)(v),                        \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_GET_XOR_RLX */

#ifndef VATOMIC16_GET_XOR
    #define VATOMIC16_GET_XOR

    #define vatomic16_get_xor(a, v)                                            \
        (vuint16_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_GET_XOR */

#ifndef VATOMIC16_GET_XOR_ACQ
    #define VATOMIC16_GET_XOR_ACQ

    #define vatomic16_get_xor_acq(a, v)                                        \
        (vuint16_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC16_GET_XOR_ACQ */

#ifndef VATOMIC16_GET_XOR_REL
    #define VATOMIC16_GET_XOR_REL

    #define vatomic16_get_xor_rel(a, v)                                        \
        (vuint16_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC16_GET_XOR_REL */

#ifndef VATOMIC16_GET_XOR_RLX
    #define VATOMIC16_GET_XOR_RLX

    #define vatomic16_get_xor_rlx(a, v)                                        \
        (vuint16_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint16_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_GET_XOR_RLX */

#ifndef VATOMIC32_GET_XOR
    #define VATOMIC32_GET_XOR

    #define vatomic32_get_xor(a, v)                                            \
        (vuint32_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_GET_XOR */

#ifndef VATOMIC32_GET_XOR_ACQ
    #define VATOMIC32_GET_XOR_ACQ

    #define vatomic32_get_xor_acq(a, v)                                        \
        (vuint32_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC32_GET_XOR_ACQ */

#ifndef VATOMIC32_GET_XOR_REL
    #define VATOMIC32_GET_XOR_REL

    #define vatomic32_get_xor_rel(a, v)                                        \
        (vuint32_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC32_GET_XOR_REL */

#ifndef VATOMIC32_GET_XOR_RLX
    #define VATOMIC32_GET_XOR_RLX

    #define vatomic32_get_xor_rlx(a, v)                                        \
        (vuint32_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint32_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_GET_XOR_RLX */

#ifndef VATOMIC64_GET_XOR
    #define VATOMIC64_GET_XOR

    #define vatomic64_get_xor(a, v)                                            \
        (vuint64_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_GET_XOR */

#ifndef VATOMIC64_GET_XOR_ACQ
    #define VATOMIC64_GET_XOR_ACQ

    #define vatomic64_get_xor_acq(a, v)                                        \
        (vuint64_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC64_GET_XOR_ACQ */

#ifndef VATOMIC64_GET_XOR_REL
    #define VATOMIC64_GET_XOR_REL

    #define vatomic64_get_xor_rel(a, v)                                        \
        (vuint64_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC64_GET_XOR_REL */

#ifndef VATOMIC64_GET_XOR_RLX
    #define VATOMIC64_GET_XOR_RLX

    #define vatomic64_get_xor_rlx(a, v)                                        \
        (vuint64_t)                                                            \
            __atomic_fetch_xor(&(a)->_v, (vuint64_t)(v),                       \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_GET_XOR_RLX */

#ifndef VATOMICSZ_GET_XOR
    #define VATOMICSZ_GET_XOR

    #define vatomicsz_get_xor(a, v)                                            \
        (vsize_t)                                                              \
            __atomic_fetch_xor(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_GET_XOR */

#ifndef VATOMICSZ_GET_XOR_ACQ
    #define VATOMICSZ_GET_XOR_ACQ

    #define vatomicsz_get_xor_acq(a, v)                                        \
        (vsize_t)                                                              \
            __atomic_fetch_xor(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICSZ_GET_XOR_ACQ */

#ifndef VATOMICSZ_GET_XOR_REL
    #define VATOMICSZ_GET_XOR_REL

    #define vatomicsz_get_xor_rel(a, v)                                        \
        (vsize_t)                                                              \
            __atomic_fetch_xor(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICSZ_GET_XOR_REL */

#ifndef VATOMICSZ_GET_XOR_RLX
    #define VATOMICSZ_GET_XOR_RLX

    #define vatomicsz_get_xor_rlx(a, v)                                        \
        (vsize_t)                                                              \
            __atomic_fetch_xor(&(a)->_v, (vsize_t)(v),                         \
                               V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_GET_XOR_RLX */

/* *****************************************************************************
 * vatomic_get_add
 * ****************************************************************************/

#ifndef VATOMIC8_GET_ADD
    #define VATOMIC8_GET_ADD

    #define vatomic8_get_add(a, v)                                             \
        (vuint8_t) __atomic_fetch_add(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_GET_ADD */

#ifndef VATOMIC8_GET_ADD_ACQ
    #define VATOMIC8_GET_ADD_ACQ

    #define vatomic8_get_add_acq(a, v)                                         \
        (vuint8_t) __atomic_fetch_add(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC8_GET_ADD_ACQ */

#ifndef VATOMIC8_GET_ADD_REL
    #define VATOMIC8_GET_ADD_REL

    #define vatomic8_get_add_rel(a, v)                                         \
        (vuint8_t) __atomic_fetch_add(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC8_GET_ADD_REL */

#ifndef VATOMIC8_GET_ADD_RLX
    #define VATOMIC8_GET_ADD_RLX

    #define vatomic8_get_add_rlx(a, v)                                         \
        (vuint8_t) __atomic_fetch_add(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_GET_ADD_RLX */

#ifndef VATOMIC16_GET_ADD
    #define VATOMIC16_GET_ADD

    #define vatomic16_get_add(a, v)                                            \
        (vuint16_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_GET_ADD */

#ifndef VATOMIC16_GET_ADD_ACQ
    #define VATOMIC16_GET_ADD_ACQ

    #define vatomic16_get_add_acq(a, v)                                        \
        (vuint16_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC16_GET_ADD_ACQ */

#ifndef VATOMIC16_GET_ADD_REL
    #define VATOMIC16_GET_ADD_REL

    #define vatomic16_get_add_rel(a, v)                                        \
        (vuint16_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC16_GET_ADD_REL */

#ifndef VATOMIC16_GET_ADD_RLX
    #define VATOMIC16_GET_ADD_RLX

    #define vatomic16_get_add_rlx(a, v)                                        \
        (vuint16_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_GET_ADD_RLX */

#ifndef VATOMIC32_GET_ADD
    #define VATOMIC32_GET_ADD

    #define vatomic32_get_add(a, v)                                            \
        (vuint32_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_GET_ADD */

#ifndef VATOMIC32_GET_ADD_ACQ
    #define VATOMIC32_GET_ADD_ACQ

    #define vatomic32_get_add_acq(a, v)                                        \
        (vuint32_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC32_GET_ADD_ACQ */

#ifndef VATOMIC32_GET_ADD_REL
    #define VATOMIC32_GET_ADD_REL

    #define vatomic32_get_add_rel(a, v)                                        \
        (vuint32_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC32_GET_ADD_REL */

#ifndef VATOMIC32_GET_ADD_RLX
    #define VATOMIC32_GET_ADD_RLX

    #define vatomic32_get_add_rlx(a, v)                                        \
        (vuint32_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_GET_ADD_RLX */

#ifndef VATOMIC64_GET_ADD
    #define VATOMIC64_GET_ADD

    #define vatomic64_get_add(a, v)                                            \
        (vuint64_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_GET_ADD */

#ifndef VATOMIC64_GET_ADD_ACQ
    #define VATOMIC64_GET_ADD_ACQ

    #define vatomic64_get_add_acq(a, v)                                        \
        (vuint64_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC64_GET_ADD_ACQ */

#ifndef VATOMIC64_GET_ADD_REL
    #define VATOMIC64_GET_ADD_REL

    #define vatomic64_get_add_rel(a, v)                                        \
        (vuint64_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC64_GET_ADD_REL */

#ifndef VATOMIC64_GET_ADD_RLX
    #define VATOMIC64_GET_ADD_RLX

    #define vatomic64_get_add_rlx(a, v)                                        \
        (vuint64_t) __atomic_fetch_add(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_GET_ADD_RLX */

#ifndef VATOMICSZ_GET_ADD
    #define VATOMICSZ_GET_ADD

    #define vatomicsz_get_add(a, v)                                            \
        (vsize_t) __atomic_fetch_add(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_GET_ADD */

#ifndef VATOMICSZ_GET_ADD_ACQ
    #define VATOMICSZ_GET_ADD_ACQ

    #define vatomicsz_get_add_acq(a, v)                                        \
        (vsize_t) __atomic_fetch_add(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICSZ_GET_ADD_ACQ */

#ifndef VATOMICSZ_GET_ADD_REL
    #define VATOMICSZ_GET_ADD_REL

    #define vatomicsz_get_add_rel(a, v)                                        \
        (vsize_t) __atomic_fetch_add(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICSZ_GET_ADD_REL */

#ifndef VATOMICSZ_GET_ADD_RLX
    #define VATOMICSZ_GET_ADD_RLX

    #define vatomicsz_get_add_rlx(a, v)                                        \
        (vsize_t) __atomic_fetch_add(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_GET_ADD_RLX */

/* *****************************************************************************
 * vatomic_get_sub
 * ****************************************************************************/

#ifndef VATOMIC8_GET_SUB
    #define VATOMIC8_GET_SUB

    #define vatomic8_get_sub(a, v)                                             \
        (vuint8_t) __atomic_fetch_sub(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC8_GET_SUB */

#ifndef VATOMIC8_GET_SUB_ACQ
    #define VATOMIC8_GET_SUB_ACQ

    #define vatomic8_get_sub_acq(a, v)                                         \
        (vuint8_t) __atomic_fetch_sub(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC8_GET_SUB_ACQ */

#ifndef VATOMIC8_GET_SUB_REL
    #define VATOMIC8_GET_SUB_REL

    #define vatomic8_get_sub_rel(a, v)                                         \
        (vuint8_t) __atomic_fetch_sub(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC8_GET_SUB_REL */

#ifndef VATOMIC8_GET_SUB_RLX
    #define VATOMIC8_GET_SUB_RLX

    #define vatomic8_get_sub_rlx(a, v)                                         \
        (vuint8_t) __atomic_fetch_sub(                                         \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC8_GET_SUB_RLX */

#ifndef VATOMIC16_GET_SUB
    #define VATOMIC16_GET_SUB

    #define vatomic16_get_sub(a, v)                                            \
        (vuint16_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC16_GET_SUB */

#ifndef VATOMIC16_GET_SUB_ACQ
    #define VATOMIC16_GET_SUB_ACQ

    #define vatomic16_get_sub_acq(a, v)                                        \
        (vuint16_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC16_GET_SUB_ACQ */

#ifndef VATOMIC16_GET_SUB_REL
    #define VATOMIC16_GET_SUB_REL

    #define vatomic16_get_sub_rel(a, v)                                        \
        (vuint16_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC16_GET_SUB_REL */

#ifndef VATOMIC16_GET_SUB_RLX
    #define VATOMIC16_GET_SUB_RLX

    #define vatomic16_get_sub_rlx(a, v)                                        \
        (vuint16_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC16_GET_SUB_RLX */

#ifndef VATOMIC32_GET_SUB
    #define VATOMIC32_GET_SUB

    #define vatomic32_get_sub(a, v)                                            \
        (vuint32_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC32_GET_SUB */

#ifndef VATOMIC32_GET_SUB_ACQ
    #define VATOMIC32_GET_SUB_ACQ

    #define vatomic32_get_sub_acq(a, v)                                        \
        (vuint32_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC32_GET_SUB_ACQ */

#ifndef VATOMIC32_GET_SUB_REL
    #define VATOMIC32_GET_SUB_REL

    #define vatomic32_get_sub_rel(a, v)                                        \
        (vuint32_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC32_GET_SUB_REL */

#ifndef VATOMIC32_GET_SUB_RLX
    #define VATOMIC32_GET_SUB_RLX

    #define vatomic32_get_sub_rlx(a, v)                                        \
        (vuint32_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC32_GET_SUB_RLX */

#ifndef VATOMIC64_GET_SUB
    #define VATOMIC64_GET_SUB

    #define vatomic64_get_sub(a, v)                                            \
        (vuint64_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMIC64_GET_SUB */

#ifndef VATOMIC64_GET_SUB_ACQ
    #define VATOMIC64_GET_SUB_ACQ

    #define vatomic64_get_sub_acq(a, v)                                        \
        (vuint64_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMIC64_GET_SUB_ACQ */

#ifndef VATOMIC64_GET_SUB_REL
    #define VATOMIC64_GET_SUB_REL

    #define vatomic64_get_sub_rel(a, v)                                        \
        (vuint64_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMIC64_GET_SUB_REL */

#ifndef VATOMIC64_GET_SUB_RLX
    #define VATOMIC64_GET_SUB_RLX

    #define vatomic64_get_sub_rlx(a, v)                                        \
        (vuint64_t) __atomic_fetch_sub(                                        \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMIC64_GET_SUB_RLX */

#ifndef VATOMICSZ_GET_SUB
    #define VATOMICSZ_GET_SUB

    #define vatomicsz_get_sub(a, v)                                            \
        (vsize_t) __atomic_fetch_sub(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_SEQ_CST))

#endif /* VATOMICSZ_GET_SUB */

#ifndef VATOMICSZ_GET_SUB_ACQ
    #define VATOMICSZ_GET_SUB_ACQ

    #define vatomicsz_get_sub_acq(a, v)                                        \
        (vsize_t) __atomic_fetch_sub(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_ACQUIRE))

#endif /* VATOMICSZ_GET_SUB_ACQ */

#ifndef VATOMICSZ_GET_SUB_REL
    #define VATOMICSZ_GET_SUB_REL

    #define vatomicsz_get_sub_rel(a, v)                                        \
        (vsize_t) __atomic_fetch_sub(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELEASE))

#endif /* VATOMICSZ_GET_SUB_REL */

#ifndef VATOMICSZ_GET_SUB_RLX
    #define VATOMICSZ_GET_SUB_RLX

    #define vatomicsz_get_sub_rlx(a, v)                                        \
        (vsize_t) __atomic_fetch_sub(                                          \
            &(a)->_v, (v), V_ACTIVATE_BUILTIN_BARRIER(__ATOMIC_RELAXED))

#endif /* VATOMICSZ_GET_SUB_RLX */

#endif /* VATOMIC_BUILTINS_H */
